{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.readers import ExtractiveReader\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from haystack.telemetry import tutorial_running\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"Data\")\n",
    "EXCEL_FILE = os.path.join(DATA_DIR, \"query_responses.xlsx\")\n",
    "FEEDBACK_FILE = os.path.join(DATA_DIR, \"feedback_dataset.json\")\n",
    "PROMPT_VISUALISATION_FILE = os.path.join(DATA_DIR, \"prompt_visualisation.txt\")\n",
    "PROCESSED_CONTENT_FILE = os.path.join(DATA_DIR, \"processed_content.txt\")\n",
    "UPLOADED_FILE = os.path.join(DATA_DIR, \"uploaded_document.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_htm_files():\n",
    "    \"\"\"\n",
    "    Recursively finds all .htm files in the DATA_DIR and its subdirectories.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of full file paths.\n",
    "    \"\"\"\n",
    "    htm_files = []\n",
    "    for root, _, files in os.walk(DATA_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".htm\"):\n",
    "                full_path = os.path.join(root, file)  # Get the absolute path\n",
    "                htm_files.append(full_path)  \n",
    "\n",
    "    return htm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(soup):\n",
    "    # Extract only meaningful paragraph text\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 20]  # Exclude very short text\n",
    "    clean_text = \"\\n\\n\".join(paragraphs)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_table(table_text):\n",
    "    \"\"\"\n",
    "    Reformats the extracted table text into a structured and retrievable format.\n",
    "\n",
    "    Args:\n",
    "        table_text (str): Raw extracted table text.\n",
    "\n",
    "    Returns:\n",
    "        str: Reformatted text suitable for retrieval.\n",
    "    \"\"\"\n",
    "    rows = table_text.split(\"\\n\")\n",
    "    reformatted_lines = []\n",
    "    \n",
    "    for row in rows:\n",
    "        # Match table rows that contain data (ignoring separators like \"+----+\")\n",
    "        match = re.match(r\"\\|\\s*(\\d+)\\s*\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|\", row)\n",
    "        if match:\n",
    "            _, key, value = match.groups()\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # Ensure meaningful values exist before adding\n",
    "            if key and value and value.lower() != \"none\":\n",
    "                reformatted_lines.append(f\"{key}: {value}\")\n",
    "\n",
    "    return \"\\n\".join(reformatted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(soup):\n",
    "    tables = soup.find_all(\"table\")\n",
    "    formatted_tables = []\n",
    "    \n",
    "    for table in tables:\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cols = [col.get_text(strip=True) for col in row.find_all([\"td\", \"th\"])]\n",
    "            rows.append(cols)\n",
    "        \n",
    "        # Flatten row values for filtering irrelevant tables\n",
    "        flat_rows = [item for sublist in rows for item in sublist]\n",
    "        if set(flat_rows) == {\"Back\", \"Forward\"}:\n",
    "            continue\n",
    "        \n",
    "        # Convert extracted table to DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Convert to readable text using tabulate\n",
    "        formatted_table = tabulate(df, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "        # Apply reformatting for better retrieval\n",
    "        structured_table = reformat_table(formatted_table)\n",
    "\n",
    "        formatted_tables.append(structured_table)\n",
    "\n",
    "    return \"\\n\\n\".join(formatted_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(soup):\n",
    "    # Extract lists properly\n",
    "    lists = []\n",
    "    for ul in soup.find_all(\"ul\"):\n",
    "        items = [li.get_text(strip=True) for li in ul.find_all(\"li\")]\n",
    "        lists.append(items)\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_content(selectedOptions=None):\n",
    "    \"\"\"\n",
    "    Load and process all .htm files from the base directory.\n",
    "    \"\"\"\n",
    "    htm_files = _list_htm_files()\n",
    "    logging.info(f\"Found {len(htm_files)} .htm files.\")\n",
    "        \n",
    "    if selectedOptions is None:\n",
    "        selectedOptions = [\"text\", \"table\", \"list\"]\n",
    "        \n",
    "    # initialise empty training web documents.\n",
    "    web_documents = []\n",
    "        \n",
    "    page_texts = []\n",
    "\n",
    "    for file_path in htm_files:\n",
    "        try:\n",
    "            with open(file_path, encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                    \n",
    "                # ignore the redundant header section from content\n",
    "                content = content[content.find(\"<body>\")+6:content.find(\"</body>\")]\n",
    "                    \n",
    "                soup = BeautifulSoup(content, \"html.parser\")\n",
    "                    \n",
    "                page_links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "                                                \n",
    "                \n",
    "                clean_text = extract_text(soup)\n",
    "                    \n",
    "                formatted_table = extract_table(soup)\n",
    "                    \n",
    "                lists = extract_list(soup)\n",
    "                        \n",
    "                page_text = f\"\"\"\n",
    "                    \n",
    "                Tables: \n",
    "                ---\n",
    "                {formatted_table}\n",
    "                ---\n",
    "                    \n",
    "                Text:\n",
    "                ---\n",
    "                {clean_text}\n",
    "                ---\n",
    "                    \n",
    "                List:\n",
    "                ---\n",
    "                {lists}\n",
    "                ---\n",
    "                \"\"\"\n",
    " \n",
    "                page_texts.append(page_text)\n",
    "                    \n",
    "                page_data = {\n",
    "                    'text': page_text,\n",
    "                    'link': page_links\n",
    "                }\n",
    "                    \n",
    "                document = Document(\n",
    "                    content = page_data['text']\n",
    "                )\n",
    "\n",
    "                if file_path.endswith(\"GEO_Limits.htm\"):\n",
    "                    print(f\"Content: {document.content}\")\n",
    "                    \n",
    "                web_documents.append(document)\n",
    "                \n",
    "        except UnicodeDecodeError:\n",
    "            logging.error(f\"Could not read the file {file_path}. Check the file encoding.\")\n",
    "\n",
    "    return web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "                    \n",
      "                Tables: \n",
      "                ---\n",
      "                Types: Limits\n",
      "Number of curves: 450\n",
      "Size of curve units: 24\n",
      "Size of curve name: 90\n",
      "Number of pen definitions: 20\n",
      "Curve selection name: 60\n",
      "Curve to lithology name: 50\n",
      "Curve to lithology lithology types: 10\n",
      "Data points per curve: Unlimited\n",
      "Computed curve parameters: 250\n",
      "Size of computed curve parameters name: 12\n",
      "Computed curve expressions: 300\n",
      "Size of computed curve expressions name: 25\n",
      "Size of computed curve parameter description: 150\n",
      "Number of 'curves for surfaces' definitions: 10\n",
      "Number of curve synonym-pairs: 500\n",
      "Number of tracks: 200\n",
      "Number of qualitative tracks: 30\n",
      "Size of track name: 75\n",
      "Number of curve shades per plot: 250\n",
      "Number of zones per curve shade: 50\n",
      "Curve shade name length: 20\n",
      "Number of data files: Unlimited\n",
      "Columns per data file: 450\n",
      "Size of file name (including the path names): 255\n",
      "Size of file ID: 9\n",
      "Number of file ID: 100\n",
      "Auto file load definition name: 40\n",
      "Number of mnemonics per file: 100\n",
      "Number of mnemonics per plot: 600\n",
      "Size of curve mnemonic: 32\n",
      "Size of file mnemonic value: 250\n",
      "Size of plot mnemonic value: 1000\n",
      "Size of mnemonic description: 40\n",
      "Number of free format text blocks per plot: 4500\n",
      "Number of characters per free format text block: 250\n",
      "Number of free format text's related to a symbol: 6\n",
      "Number of free format text tags: 50\n",
      "Free format text tag size: 31\n",
      "Number of track text blocks per plot: 6000\n",
      "Number of characters per track text block: 32000\n",
      "Number of qualitative tracks: 10\n",
      "Number of graduations per qualitative track: 20\n",
      "Qualitative track name length: 32\n",
      "Qualitative track abbreviation length: 8\n",
      "Number of tables: 100\n",
      "Number of rows in 'operations diary' type table: 4320\n",
      "Number of rows in 'normal' and 'operations remarks' type table: 32000\n",
      "Number of fields in a row`: 20\n",
      "Columns per table: 4320\n",
      "Size of table name: 29\n",
      "Size of table ID: 12\n",
      "Size of table column heading: 29\n",
      "Size of postfix: 20\n",
      "Characters for all columns in table: 10000\n",
      "Characters for an individual cell: 1999\n",
      "Number of lithology types: 450\n",
      "Number of lithology sections per plot: 20000\n",
      "%Litho track per plot: 3\n",
      "Number of lithology types per %Litho track: 10\n",
      "Number of modifier types: 450\n",
      "Number of modifiers per plot: 20000\n",
      "Number of symbol types: 1000\n",
      "Number of symbols per plot: 10000\n",
      "Number of lines per plot: 750\n",
      "Number of header & trailers specification files: 100\n",
      "Number of specifications to make a plot header: 50\n",
      "Number of specifications to make a plot trailer: 50\n",
      "Tadpole definitions: 5\n",
      "Tadpole definitions name: 16\n",
      "Minimum dip value: 0\n",
      "Maximum dip value: 90\n",
      "Minimum azimuth value: 0\n",
      "Maximum azimuth value: 360\n",
      "Maximum dip types: 64\n",
      "Maximum length of dip type name: 63\n",
      "Maximum tadpole definition templates: 50\n",
      "Minimum zoom: 0.1\n",
      "Maximum zoom: 10\n",
      "Size of plot description: 29\n",
      "Number of CGM fonts for font mapping: 20\n",
      "Size of CGM font name: 30\n",
      "Scale settings: 23\n",
      "Password length: 16\n",
      "Size of image name: 32\n",
      "User ID's per plot: 64\n",
      "Vendor ID's per plot: 64\n",
      "Maximum layouts per ODF: 19\n",
      "Number of points per polygon in VOB: 20\n",
      "Number of different fonts for VOB: 50\n",
      "Size of text in VOB: 300\n",
      "Memory for all bitmaps and VOBs: 300 KB\n",
      "Maximum imaging tools: 10\n",
      "Maximum pads per tool: 10\n",
      "Maximum sensors per pad: 64\n",
      "Maximum name length: 63\n",
      "Maximum significant decimals: 4\n",
      "Number of query definitions per zone type: 75\n",
      "Correlation items: 50\n",
      "                ---\n",
      "                    \n",
      "                Text:\n",
      "                ---\n",
      "                *Maximize screen to view table of contents*\n",
      "\n",
      "The following restrictions currently apply within each session of GEO, or instance ofODFfile.\n",
      "\n",
      "Once a limit is reached, the system may beep and/or display an error message for that object type.\n",
      "\n",
      "To check whether the following limits have been exceeded, click Statistics from the Help menu.\n",
      "\n",
      "The Statistics dialog box displays the number of modifiers, lithologies, symbols, texts and lines, which are being used in the currently-open ODF.\n",
      "                ---\n",
      "                    \n",
      "                List:\n",
      "                ---\n",
      "                []\n",
      "                ---\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "documents = _load_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a set to track unique documents based on content and meta\n",
    "unique_docs = {}\n",
    "for doc in documents:\n",
    "    doc_key = (doc.content.strip(), tuple(doc.meta.items()))  # Normalize content & meta\n",
    "    if doc_key not in unique_docs:\n",
    "        unique_docs[doc_key] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to a list of unique Document objects\n",
    "# documents = list(unique_docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Meta: {'url': 'https://en.wikipedia.org/wiki/Hanging_Gardens_of_Babylon', '_split_id': 8}\n",
      "\n",
      "Top result:\n",
      "[36] There was a tradition of Assyrian royal garden building. King Ashurnasirpal\n",
      "II (883â€“859 BC) had created a canal, which cut through the mountains. Fruit tree\n",
      "orchards were planted. Also mentioned were pines, cypresses and junipers; almond\n",
      "trees, date trees, ebony, rosewood, olive, oak, tamarisk, walnut, terebinth,\n",
      "ash, fir, pomegranate, pear, quince, fig, and grapes. A sculptured wall panel of\n",
      "Assurbanipal shows the garden in its maturity. One original panel[37] and the\n",
      "drawing of another[38] are held by the British Museum, although neither is on\n",
      "public display. Several features mentioned by the classical authors are\n",
      "discernible on these contemporary images.  Assyrian wall relief showing gardens\n",
      "in Nineveh Of Sennacherib's palace, he mentions the massive limestone blocks\n",
      "that reinforce the flood defences. Parts of the palace were excavated by Austin\n",
      "Henry Layard in the mid-19th century. His citadel plan shows contours which\n",
      "would be consistent with Sennacherib's garden, but its position has not been\n",
      "confirmed. The area has been used as a military base in recent times, making it\n",
      "difficult to investigate further. The irrigation of such a garden demanded an\n",
      "upgraded water supply to the city of Nineveh. The canals stretched over 50\n",
      "kilometres (31Â mi) into the mountains.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import textwrap\n",
    "\n",
    "# Assuming `documents` is the list of Document objects\n",
    "random_doc = random.choice(documents)  # Select a random document\n",
    "\n",
    "# Format the output for readability\n",
    "wrapped_content = textwrap.fill(random_doc.content, width=80)\n",
    "\n",
    "# Print the selected document's meta and wrapped content\n",
    "print(f\"Random Meta: {random_doc.meta}\\n\")\n",
    "print(f\"Top result:\\n{wrapped_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc39631574754e8db63b1020d21b57a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 151}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "indexing_pipeline.add_component(instance=SentenceTransformersDocumentEmbedder(model=model), name=\"embedder\")\n",
    "indexing_pipeline.add_component(instance=DocumentWriter(document_store=document_store), name=\"writer\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "indexing_pipeline.run({\"documents\": documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.readers import ExtractiveReader\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "reader = ExtractiveReader()\n",
    "reader.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000002A2D4CD81D0>\n",
       "ðŸš… Components\n",
       "  - embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - reader: ExtractiveReader\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> reader.documents (List[Document])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractive_qa_pipeline = Pipeline()\n",
    "\n",
    "extractive_qa_pipeline.add_component(instance=SentenceTransformersTextEmbedder(model=model), name=\"embedder\")\n",
    "extractive_qa_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
    "extractive_qa_pipeline.add_component(instance=reader, name=\"reader\")\n",
    "\n",
    "extractive_qa_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "extractive_qa_pipeline.connect(\"retriever.documents\", \"reader.documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f7238755ef4280bea287fdd0385140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Who was Pliney the elder\"\n",
    "answers = extractive_qa_pipeline.run(\n",
    "    data={\"embedder\": {\"text\": query}, \"retriever\": {\"top_k\": 5}, \"reader\": {\"query\": query, \"top_k\": 2}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Roman writer\n",
      "Answer: a Roman author\n",
      "Answer: None\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(answers['reader']['answers'])):\n",
    "    print(f\"Answer: {answers['reader']['answers'][i].data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
